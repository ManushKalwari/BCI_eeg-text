# EEG â†’ Text (BCI_eeg-text)

**Modular EEG-to-Text decoding** that:
1) self-supervises an EEG encoder,  
2) aligns EEG â†” text embeddings via a CLIP-style contrastive loss, and  
3) decodes with a BART-based autoregressive head.

> TL;DR: We align noisy EEG to language embeddings, then generate text with a pretrained LM. The pipeline is designed for **generalization without teacher forcing** at inference. 
---

## ğŸš€ Key Ideas
- **EEG encoder pretraining** with masked autoencoding to learn local/global temporal structure. 
- **Contrastive alignment**: project EEG and BERT embeddings to a shared space; optimize symmetric InfoNCE with temperature.  
- **Decoder**: freeze the EEG encoder, condition a **BART** decoder on the projected EEG vector; train autoregressively. 
- **No teacher forcing at inference** (to mirror real deployments). 

---

## ğŸ“¦ Repo Structure
BCI_eeg-text/
â”œâ”€ data/ # (expected) ZuCo 2.0 processed tensors / metadata
â”œâ”€ eeg_pretrain/ # masked autoencoder for EEG
â”œâ”€ align/ # EEGâ†”Text contrastive alignment (CLIP-style)
â”œâ”€ decode/ # BART-based autoregressive decoder
â”œâ”€ scripts/ # end-to-end run scripts
â”œâ”€ utils/ # common loaders, metrics, logging
â””â”€ README.md


_If your file layout differs, adjust these paths in the commands below._

---

## ğŸ§  Dataset: ZuCo 2.0 (Natural Reading)
We use **ZuCo 2.0** (EEG during natural reading). EEG sampled with 128-channel BioSemi at 500 Hz; we use the NR subset. Download per the dataset license and place under `data/`. 

Preprocessing (high level):
- segment time series to overlapping patches;  
- artifact removal + normalization;  
- align word-level EEG segments with text;  
- produce word/phrase text labels.

---

## ğŸ› ï¸ Setup
```bash
# Python 3.10+ recommended
python -m venv .venv && source .venv/bin/activate

pip install -r requirements.txt
# expects: torch, transformers, datasets, numpy, scipy, scikit-learn, einops, tqdm, matplotlib, wandb (optional)
